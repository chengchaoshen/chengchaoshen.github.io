<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        
        <script type="text/javascript" src="./jquery.min.js"></script>

        <link rel="stylesheet" type="text/css" href="./mystyle.css">

        <title>沈成超 个人主页</title>
    </head>

    <body>
        <div class="container">
            <!-- --------------- Introduction --------------- -->
            <table class="intro">
                <tbody>
                    <tr>
                        <td width=20%>
                            <p style="text-align:center"><font face="Arial"><img src="./images/avatar.png" width=180px></font></p>
                        </td>
                        <td width=50%>
                            <p style="font-size: 18pt;"><b>沈成超</b></p>

                            <p>特聘副教授，中南大学计算机学院</p>

                            <!-- <p>
                                邮箱: 
                                <img src="./images/em.png" height=22px style="vertical-align: bottom;">
                            </p> -->
                            <img src="./images/em.png" height=22px style="vertical-align: bottom;">

                            <p>
                                研究方向: 自监督学习/无监督学习, 零样本学习/小样本学习, 迁移学习, 联邦学习, 可解释性, 模型压缩
                            </p>

                            <p>
                                [<a href="https://scholar.google.com/citations?user=lQ8tqZ4AAAAJ" target="_blank">Google Scholar</a>] 
                                [<a href="https://dblp.org/pid/217/3551"  target="_blank">DBLP</a>] 
                                [<a href="https://github.com/visresearch" target="_blank">Github</a>] 
                            </p>
                            
                        </td>

                        <td width=30%>
                            <p style="text-align:right;font-size: 18pt;"><a href="./index.html" target="_blank">[English]</a></p>
                            <div class="tocright"><div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading">
                                <input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none">
                                <div class="toctitle" lang="en" dir="ltr">
                                    <div id="mw-toc-heading"><b>导航目录</b></div>
                                    <span class="toctogglespan"
                                        <label class="toctogglelabel" for="toctogglecheckbox"></label>
                                    </span>
                                </div>
                                <ul>
                                    <li class="toclevel-1 tocsection-2"><a href="#publications"><span class="tocnumber">1.</span> <span class="toctext">发表论文</span></a></li>
                                    <!-- <li class="toclevel-1 tocsection-1"><a href="#education"><span class="tocnumber">2.</span> <span class="toctext">教育经历</span></a></li> -->
                                    <li class="toclevel-1 tocsection-3"><a href="#services"><span class="tocnumber">2.</span> <span class="toctext">学术服务</span></a></li>
                                </ul>
                            </div>
                            <br><br><br><br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <!-- --------------- Publications --------------- -->
            <div class="publications" id="publications">
                <!-- <br> -->
                <!-- <div class="title">Publications</div> -->
                <h3 class="title">代表论文</h3>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/shen2023inter.png"></img></div>
                        <dt class="ptitle">Inter-Instance Similarity Modeling for Contrastive Learning</dt>
                        <dd><b class='me'>Chengchao Shen</b>, Dawei Liu, Hao Tang, Zhe Qu, Jianxin Wang</dd>
                        <dd>Under Review (<b>代表作</b>), 2023</dd>
                        <dd>
                            [<a href="https://arxiv.org/abs/2306.12243" target="_blank">arXiv</a>]
                            [<a href="https://github.com/visresearch/patchmix" target="_blank">code</a>]
                            [<a href="https://zhuanlan.zhihu.com/p/639240952" target="_blank">blog+cn</a>]
                            [<a href="" target="_blank">blog+en</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@article{shen2023inter,
  author  = {Shen, Chengchao and Liu, Dawei and Tang, Hao and Qu, Zhe and Wang, Jianxin},
  title   = {Inter-Instance Similarity Modeling for Contrastive Learning},
  journal = {arXiv preprint arXiv:2306.12243},
  year    = {2023},
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>The existing contrastive learning methods widely adopt one-hot instance discrimination as pretext task for self-supervised learning, which inevitably neglects rich inter-instance similarities among natural images, then leading to potential representation degeneration. In this paper, we propose a novel image mix method, PatchMix, for contrastive learning in Vision Transformer (ViT), to model inter-instance similarities among images. Following the nature of ViT, we randomly mix multiple images from mini-batch in patch level to construct mixed image patch sequences for ViT. Compared to the existing sample mix methods, our PatchMix can flexibly and efficiently mix more than two images and simulate more complicated similarity relations among natural images. In this manner, our contrastive framework can significantly reduce the gap between contrastive objective and ground truth in reality. Experimental results demonstrate that our proposed method significantly outperforms the previous state-of-the-art on both ImageNet-1K and CIFAR datasets, e.g., 3.0% linear accuracy improvement on ImageNet-1K and 8.7% kNN accuracy improvement on CIFAR100. Moreover, our method achieves the leading transfer performance on downstream tasks, object detection and instance segmentation on COCO dataset.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/visresearch/patchmix.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                        
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/shen2023asymmetric.png"></img></div>
                        <dt class="ptitle">Asymmetric Patch Sampling for Contrastive Learning</dt>
                        <dd><b class='me'>Chengchao Shen</b>, Jianzhong Chen, Shu Wang, Hulin Kuang, Jin Liu, Jianxin Wang</dd>
                        <dd>Under Review (<b>代表作</b>), 2023</dd>
                        <dd>
                            [<a href="https://arxiv.org/abs/2306.02854" target="_blank">arXiv</a>]
                            [<a href="https://github.com/visresearch/aps" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@article{shen2023asymmetric,
      title={Asymmetric Patch Sampling for Contrastive Learning}, 
      author={Shen, Chengchao and Chen, Jianzhong and Wang, Shu and Kuang, Hulin and Liu, Jin and Wang, Jianxin},
      journal={arXiv preprint arXiv:2306.02854},
      year={2023}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Asymmetric appearance between positive pair effectively reduces the risk of representation degradation in contrastive learning. However, there are still a mass of appearance similarities between positive pair constructed by the existing methods, which inhibits the further representation improvement. In this paper, we propose a novel asymmetric patch sampling strategy for contrastive learning, to further boost the appearance asymmetry for better representations. Specifically, dual patch sampling strategies are applied to the given image, to obtain asymmetric positive pairs. First, sparse patch sampling is conducted to obtain the first view, which reduces spatial redundancy of image and allows a more asymmetric view. Second, a selective patch sampling is proposed to construct another view with large appearance discrepancy relative to the first one. Due to the inappreciable appearance similarity between positive pair, the trained model is encouraged to capture the similarity on semantics, instead of low-level ones. Experimental results demonstrate that our proposed method significantly outperforms the existing self-supervised methods on both ImageNet-1K and CIFAR dataset, e.g., 2.5% finetune accuracy improvement on CIFAR100. Furthermore, our method achieves state-of-the-art performance on downstream tasks, object detection and instance segmentation on COCO. Additionally, compared to other self-supervised methods, our method is more efficient on both memory and computation during training.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/visresearch/aps.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                        
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/sheng2023modeling.png"></img></div>
                        <dt class="ptitle">Modeling Global Distribution for Federated Learning with Label Distribution Skew</dt>
                        <dd>Tao Sheng, <b class='me'>Chengchao Shen*</b>, Yuan Liu, Yeyu Ou, Zhe Qu, Yixiong Liang, Jianxin Wang</dd>
                        <dd>Pattern Recognition (<b>中科院一区</b>, IF=8.5), 2023</dd>
                        <dd>
                            [<a href="https://www.sciencedirect.com/science/article/pii/S0031320323004223" target="_blank">paper</a>]
                            [<a href="https://arxiv.org/abs/2212.08883" target="_blank">arXiv</a>]
                            [<a href="https://github.com/Sheng-T/FedMGD" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@article{sheng2023modeling,
    author    = {Sheng, Tao and Shen, Chengchao and Liu, Yuan and Ou, Yeyu and Qu, Zhe and Wang, Jianxin},
    title     = {Modeling Global Distribution for Federated Learning with Label Distribution Skew},
    journal   = {Pattern Recognition},
    volume = {143},
    pages = {109724},
    year      = {2023}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Federated learning achieves joint training of deep models by connecting decentralized data sources, which can significantly mitigate the risk of privacy leakage. However, in a more general case, the distributions of labels among clients are different, called ``label distribution skew''. Directly applying conventional federated learning without consideration of label distribution skew issue significantly hurts the performance of the global model. To this end, we propose a novel federated learning method, named FedMGD, to alleviate the performance degradation caused by the label distribution skew issue. It introduces a global Generative Adversarial Network to model the global data distribution without access to local datasets, so the global model can be trained using the global information of data distribution without privacy leakage. The experimental results demonstrate that our proposed method significantly outperforms the state-of-the-art on several public benchmarks.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/Sheng-T/FedMGD.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                        
                    </dl>
                </div>
                
                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/shen2021training.png"></img></div>
                        <dt class="ptitle">Training Generative Adversarial Networks in One Stage</dt>
                        <dd><b class='me'>Chengchao Shen</b>, Youtan Yin, Xinchao Wang, Xubin Li, Jie Song, Mingli Song</dd>
                        <dd>The IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>, <b>CCF A</b>), 2021</dd>
                        <dd>
                            [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Training_Generative_Adversarial_Networks_in_One_Stage_CVPR_2021_paper.pdf" target="_blank">paper</a>]
                            [<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Shen_Training_Generative_Adversarial_CVPR_2021_supplemental.pdf" target="_blank">supp</a>]
                            [<a href="http://arxiv.org/abs/2103.00430" target="_blank">arXiv</a>]
                            [<a href="https://github.com/zju-vipa/OSGAN" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{shen2021training,
    author    = {Shen, Chengchao and Yin, Youtan and Wang, Xinchao and Li, Xubin and Song, Jie and Song, Mingli},
    title     = {Training Generative Adversarial Networks in One Stage},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3350--3360}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Generative Adversarial Networks (GANs) have demonstrated unprecedented success in various image generation tasks. The encouraging results, however, come at the price of a cumbersome training process, during which the generator and discriminator are alternately updated in two stages. In this paper, we investigate a general training scheme that enables training GANs efficiently in only one stage. Based on the adversarial losses of the generator and discriminator, we categorize GANs into two classes, Symmetric GANs and Asymmetric GANs, and introduce a novel gradient decomposition method to unify the two, allowing us to train both classes in one stage and hence alleviate the training effort. We also computationally analyze the efficiency of the proposed method, and empirically demonstrate that, the proposed method yields a solid 1.5× acceleration across various datasets and network architectures. Furthermore, we show that the proposed method is readily applicable to other adversarial-training scenarios, such as data-free knowledge distillation. </p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/zju-vipa/OSGAN.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                        
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/shen2021progressive.png"></img></div>
                        <dt class="ptitle">Progressive Network Grafting for Few-Shot Knowledge Distillation</dt>
                        <dd><b class='me'>Chengchao Shen</b>, Xinchao Wang, Youtan Yin, Jie Song, Sihui Luo, Mingli Song</dd>
                        <dd>AAAI Conference on Artificial Intelligence (<b>AAAI</b>, <b>CCF A</b>), 2021</dd>
                        <dd>
                            [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16356/16163" target="_blank">paper</a>]
                            [<a href="https://arxiv.org/abs/2012.04915" target="_blank">arXiv</a>]
                            [<a href="https://github.com/zju-vipa/NetGraft" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{shen2021progressive,
    title={Progressive Network Grafting for Few-Shot Knowledge Distillation},
    author={Shen, Chengchao and Wang, Xinchao and Yin, Youtan and Song, Jie and Luo, Sihui and Song, Mingli},
    booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/16356},  
    year={2021},
    month={May}, 
    volume={35}, 
    pages={2541--2549} 
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Knowledge distillation has demonstrated encouraging performances in deep model compression. Most existing approaches, however, require massive labeled data to accomplish the knowledge transfer, making the model compression a cumbersome and costly process. In this paper, we investigate the practical few-shot knowledge distillation scenario, where we assume only a few samples without human annotations are available for each category. To this end, we introduce a principled dual-stage distillation scheme tailored for few-shot data. In the first step, we graft the student blocks one by one onto the teacher, and learn the parameters of the grafted block intertwined with those of the other teacher blocks. In the second step, the trained student blocks are progressively connected and then together grafted onto the teacher network, allowing the learned student blocks to adapt themselves to each other and eventually replace the teacher network. Experiments demonstrate that our approach, with only a few unlabeled samples, achieves gratifying results on CIFAR10, CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances are even on par with those of knowledge distillation schemes that utilize the full datasets.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/zju-vipa/NetGraft.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                        
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/shen2019customizing.png"></img></div>
                        <dt class="ptitle">Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation</dt>
                        <dd><b class='me'>Chengchao Shen</b>, Mengqi Xue, Xinchao Wang, Jie Song, Li Sun, Mingli Song</dd>
                        <dd>The IEEE International Conference on Computer Vision (<b>ICCV</b>, <b>CCF A</b>), 2019</dd>
                        <dd>
                            [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.pdf" target="_blank">paper</a>]
                            [<a href="https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Shen_Customizing_Student_Networks_ICCV_2019_supplemental.pdf" target="_blank">supp</a>]
                            [<a href="https://arxiv.org/abs/1908.07121" target="_blank">arXiv</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{shen2019customizing,
    author    = {Shen, Chengchao and Xue, Mengqi and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli},
    title     = {Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month     = {October},
    pages     = {3504--3513},
    year      = {2019}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>A massive number of well-trained deep networks have been released by developers online. These networks may focus on different tasks and in many cases are optimized for different datasets. In this paper, we study how to exploit such heterogeneous pre-trained networks, known as teachers, so as to train a customized student network that tackles a set of selective tasks defined by the user. We assume no human annotations are available, and each teacher may be either single- or multi-task. To this end, we introduce a dual-step strategy that first extracts the task-specific knowledge from the heterogeneous teachers sharing the same sub-task, and then amalgamates the extracted knowledge to build the student network. To facilitate the training, we employ a selective learning scheme where, for each unlabelled sample, the student learns adaptively from only the teacher with the least prediction ambiguity. We evaluate the proposed approach on several datasets and the experimental results demonstrate that the student, learned by such adaptive knowledge amalgamation, achieves performances even better than those of the teachers.</p>
                                </div>
                            </div>
                        </dd>
                        
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/shen2019amalgamating.png"></img></div>
                        <dt class="ptitle">Amalgamating Knowledge towards Comprehensive Classification</dt>
                        <dd><b class='me'>Chengchao Shen</b>, Xinchao Wang, Jie Song, Li Sun, Mingli Song</dd>
                        <dd>AAAI Conference on Artificial Intelligence (<b>AAAI</b>, <b>CCF A</b>), 2019</dd>
                        <dd>
                            [<a href="https://aaai.org/ojs/index.php/AAAI/article/view/4165" target="_blank">paper</a>]
                            [<a href="https://arxiv.org/abs/1811.02796" target="_blank">arXiv</a>]
                            [<a href="https://github.com/zju-vipa/KamalEngine" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{shen2019amalgamating,
    author    = {Shen, Chengchao and Wang, Xinchao and Song, Jie and Sun, Li and Song, Mingli},
    title     = {Amalgamating Knowledge towards Comprehensive Classification},
    booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
    pages     = {3068--3075},
    year      = {2019}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>With the rapid development of deep learning, there have been an unprecedentedly large number of trained deep network models available online. Reusing such trained models can significantly reduce the cost of training the new models from scratch, if not infeasible at all as the annotations used for the training original networks are often unavailable to public. We propose in this paper to study a new model-reusing task, which we term as \emph{knowledge amalgamation}. Given multiple trained teacher networks, each of which specializes in a different classification problem, the goal of knowledge amalgamation is to learn a lightweight student model capable of handling the comprehensive classification. We assume no other annotations except the outputs from the teacher models are available, and thus focus on extracting and amalgamating knowledge from the multiple teachers. To this end, we propose a pilot two-step strategy to tackle the knowledge amalgamation task, by learning first the compact feature representations from teachers and then the network parameters in a layer-wise manner so as to build the student model. We apply this approach to four public datasets and obtain very encouraging results: even without any human annotation, the obtained student model is competent to handle the comprehensive classification task and in most cases outperforms the teachers in individual sub-tasks.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/zju-vipa/KamalEngine.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                        
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/shen2018intra.png"></img></div>
                        <dt class="ptitle">Intra-class Structure Aware Networks for Screen Defect Detection</dt>
                        <dd><b class='me'>Chengchao Shen</b>, Jie Song, Sihui Luo, Li Sun, Mingli Song</dd>
                        <dd>International Conference on Neural Information Processing (<b>ICONIP</b>, <b>CCF C</b>), 2018</dd>
                        <dd>
                            [<a href="https://link.springer.com/chapter/10.1007/978-3-030-04212-7_42" target="_blank">paper</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{shen2018intra,
    title={Intra-class Structure Aware Networks for Screen Defect Detection},
    author={Shen, Chengchao and Song, Jie and Song, Shuyi and Luo, Sihui and Sun, Li and Song, Mingli},
    booktitle={International Conference on Neural Information Processing (ICONIP)},
    pages={476--485},
    year={2018},
    organization={Springer}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Typically, screen defect detection treats different types of defects as a single category and ignores the large variation among them, which may pose large difficulty to the model learning and thus lead to inferior performance. In this paper, we propose a novel network model, called Intra-class Structure Aware Networks (ISANs), to alleviate the difficulty of learning one single concept which exhibits in various forms. The proposed model introduces more neural units for the “defect” category rather than a single one, to accommodate the large variations in this category, which can significantly improve the representation power. Regularized by prior distribution of intra-class variants, our approach can learn intra-class structure of screen defect without extra fine-grained labels. Experimental results demonstrate that ISANs can effectively discriminate intra-class variants and gain significant performance improvement on screen defect detection task as well as the classification task in MNIST.</p>
                                </div>
                            </div>
                        </dd>
                        
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/song2018selective.png"></img></div>
                        <dt class="ptitle">Selective Zero-Shot Classification With Augmented Attributes</dt>
                        <dd>Jie Song, <b class='me'>Chengchao Shen</b>, Jie Lei, An-Xiang Zeng, Kairi Ou, Dacheng Tao, Mingli Song</dd>
                        <dd>The European Conference on Computer Vision (<b>ECCV</b>, <b>CCF B</b>), 2018</dd>
                        <dd>
                            [<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Jie_Song_Selective_Zero-Shot_Classification_ECCV_2018_paper.pdf" target="_blank">paper</a>]
                            [<a href="http://arxiv.org/abs/1807.07437v1" target="_blank">arXiv</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{song2018selective,
    title={Selective zero-shot classification with augmented attributes},
    author={Song, Jie and Shen, Chengchao and Lei, Jie and Zeng, An-Xiang and Ou, Kairi and Tao, Dacheng and Song, Mingli},
    booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
    pages={468--483},
    year={2018}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>In this paper, we introduce a selective zero-shot classification problem: how can the classifier avoid making dubious predictions? Existing attribute-based zero-shot classification methods are shown to work poorly in the selective classification scenario. We argue the under-complete human defined attribute vocabulary accounts for the poor performance. We propose a selective zero-shot classifier based on both the human defined and the automatically discovered residual attributes. The proposed classifier is constructed by firstly learning the defined and the residual attributes jointly. Then the predictions are conducted within the subspace of the defined attributes. Finally, the prediction confidence is measured by both the defined and the residual attributes. Experiments conducted on several benchmarks demonstrate that our classifier produces a superior performance to other methods under the risk-coverage trade-off metric.</p>
                                </div>
                            </div>       
                        </dd>
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/song2018transductive.png"></img></div>
                        <dt class="ptitle">Transductive Unbiased Embedding for Zero-Shot Learning</dt>
                        <dd>Jie Song, <b class='me'>Chengchao Shen</b>, Yezhou Yang, Yang Liu, Mingli Song</dd>
                        <dd>The IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>, <b>CCF A</b>), 2018</dd>
                        <dd>
                            [<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Transductive_Unbiased_Embedding_CVPR_2018_paper.pdf" target="_blank">paper</a>]
                            [<a href="http://arxiv.org/abs/1803.11320v1" target="_blank">arXiv</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{song2018transductive,
    author    = {Song, Jie and Shen, Chengchao and Yang, Yezhou and Liu, Yang and Song, Mingli},
    title     = {Transductive Unbiased Embedding for Zero-Shot Learning},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    pages     = {1024--1033},
    year      = {2018}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem, in which instances of unseen (target) classes tend to be categorized as one of the seen (source) classes. So they yield poor performance after being deployed in the generalized ZSL settings. In this paper, we propose a straightforward yet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate the bias problem. Our method follows the way of transductive learning, which assumes that both the labeled source images and unlabeled target images are available for training. In the semantic embedding space, the labeled source images are mapped to several fixed points specified by the source categories, and the unlabeled target images are forced to be mapped to other points specified by the target categories. Experiments conducted on AwA2, CUB and SUN datasets demonstrate that our method outperforms existing state-of-the-art approaches by a huge margin of 9.3~24.5% following generalized ZSL settings, and by a large margin of 0.2~16.2% following conventional ZSL settings.</p>
                                </div>
                            </div>       
                        </dd>
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/fang2021contrastive.png"></img></div>
                        <dt class="ptitle">Contrastive Model Inversion for Data-Free Knowledge Distillation</dt>
                        <dd>Gongfan Fang, Jie Song, Xinchao Wang, <b class='me'>Chengchao Shen</b>, Xingen Wang, Mingli Song</dd>
                        <dd>International Joint Conference on Artificial Intelligence (<b>IJCAI</b>, <b>CCF A</b>), 2021</dd>
                        <dd>
                            [<a href="https://www.ijcai.org/proceedings/2021/0327.pdf" target="_blank">paper</a>]
                            [<a href="https://arxiv.org/abs/2105.08584" target="_blank">arXiv</a>]
                            [<a href="https://github.com/zju-vipa/CMI" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{fang2021contrastive,
  title={Contrastive Model Inversion for Data-Free Knowledge Distillation},
  author={Fang, Gongfan and Song, Jie and Wang, Xinchao and Shen, Chengchao and Wang, Xingen and Song, Mingli},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2021},
  pages={2374--2380}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Model inversion, whose goal is to recover training data from a pre-trained model, has been recently proved feasible. However, existing inversion methods usually suffer from the mode collapse problem, where the synthesized instances are highly similar to each other and thus show limited effectiveness for downstream tasks, such as knowledge distillation. In this paper, we propose Contrastive Model Inversion~(CMI), where the data diversity is explicitly modeled as an optimizable objective, to alleviate the mode collapse issue. Our main observation is that, under the constraint of the same amount of data, higher data diversity usually indicates stronger instance discrimination. To this end, we introduce in CMI a contrastive learning objective that encourages the synthesizing instances to be distinguishable from the already synthesized ones in previous batches. Experiments of pre-trained models on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CMI not only generates more visually plausible instances than the state of the arts, but also achieves significantly superior performance when the generated data are used for knowledge distillation.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/zju-vipa/CMI.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                    </dl>
                </div>


                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/fang2021mosaicking.png"></img></div>
                        <dt class="ptitle">Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data</dt>
                        <dd>Gongfan Fang, Yifan Bao, Jie Song, Xinchao Wang, Donglin Xie, <b class='me'>Chengchao Shen</b>, Mingli Song</dd>
                        <dd>Advances in Neural Information Processing Systems (<b>NeurIPS</b>, <b>CCF A</b>), 2021</dd>
                        <dd>
                            [<a href="https://papers.nips.cc/paper/2021/file/63dc7ed1010d3c3b8269faf0ba7491d4-Paper.pdf" target="_blank">paper</a>]
                            [<a href="https://arxiv.org/abs/2110.15094" target="_blank">arXiv</a>]
                            [<a href="https://papers.nips.cc/paper/2021/file/63dc7ed1010d3c3b8269faf0ba7491d4-Supplemental.pdf" target="_blank">supp</a>]
                            [<a href="https://github.com/zju-vipa/MosaicKD" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{fang2021mosaicking,
    title={Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data},
    author={Fang, Gongfan and Bao, Yifan and Song, Jie and Wang, Xinchao and Xie, Donglin and Shen, Chengchao and Song, Mingli},
    booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
    year={2021}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Knowledge distillation (KD) aims to craft a compact student model that imitates the behavior of a pre-trained teacher in a target domain. Prior KD approaches, despite their gratifying results, have largely relied on the premise that in-domain data is available to carry out the knowledge transfer. Such an assumption, unfortunately, in many cases violates the practical setting, since the original training data or even the data domain is often unreachable due to privacy or copyright reasons. In this paper, we attempt to tackle an ambitious task, termed as out-of-domain knowledge distillation (OOD-KD), which allows us to conduct KD using only OOD data that can be readily obtained at a very low cost. Admittedly, OOD-KD is by nature a highly challenging task due to the agnostic domain gap. To this end, we introduce a handy yet surprisingly efficacious approach, dubbed as MosaicKD. The key insight behind MosaicKD lies in that, samples from various domains share common local patterns, even though their global semantic may vary significantly; these shared local patterns, in turn, can be re-assembled analogous to mosaic tiling, to approximate the in-domain data and to further alleviating the domain discrepancy. In MosaicKD, this is achieved through a four-player min-max game, in which a generator, a discriminator, a student network, are collectively trained in an adversarial manner, partially under the guidance of a pre-trained teacher. We validate MosaicKD over classification and semantic segmentation tasks across various benchmarks, and demonstrate that it yields results much superior to the state-of-the-art counterparts on OOD data.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/zju-vipa/MosaicKD.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/fang2019data.png"></img></div>
                        <dt class="ptitle">Data-Free Adversarial Distillation</dt>
                        <dd>Gongfan Fang, Jie Song, <b class='me'>Chengchao Shen</b>, Xinchao Wang, Da Chen, Mingli Song</dd>
                        <dd>ArXiv:1912.11006, 2019</dd>
                        <dd>
                            [<a href="https://arxiv.org/abs/1912.11006" target="_blank">arXiv</a>]
                            [<a href="https://github.com/VainF/Data-Free-Adversarial-Distillation" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@article{fang2019data,
    title={Data-free adversarial distillation},
    author={Fang, Gongfan and Song, Jie and Shen, Chengchao and Wang, Xinchao and Chen, Da and Song, Mingli},
    journal={arXiv preprint arXiv:1912.11006},
    year={2019}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Knowledge Distillation (KD) has made remarkable progress in the last few years and become a popular paradigm for model compression and knowledge transfer. However, almost all existing KD algorithms are data-driven, i.e., relying on a large amount of original training data or alternative data, which is usually unavailable in real-world scenarios. In this paper, we devote ourselves to this challenging problem and propose a novel adversarial distillation mechanism to craft a compact student model without any real-world data. We introduce a model discrepancy to quantificationally measure the difference between student and teacher models and construct an optimizable upper bound. In our work, the student and the teacher jointly act the role of the discriminator to reduce this discrepancy, when a generator adversarially produces some "hard samples" to enlarge it. Extensive experiments demonstrate that the proposed data-free method yields comparable performance to existing data-driven methods. More strikingly, our approach can be directly extended to semantic segmentation, which is more complicated than classification, and our approach achieves state-of-the-art results.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/VainF/Data-Free-Adversarial-Distillation.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/song2020depara.png"></img></div>
                        <dt class="ptitle">DEPARA: Deep Attribution Graph for Deep Knowledge Transferability</dt>
                        <dd>Jie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, <b class='me'>Chengchao Shen</b>, Feng Mao, Mingli Song</dd>
                        <dd>The IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>, <b>CCF A</b>), 2020</dd>
                        <dd>
                            [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Song_DEPARA_Deep_Attribution_Graph_for_Deep_Knowledge_Transferability_CVPR_2020_paper.pdf" target="_blank">paper</a>]
                            [<a href="http://arxiv.org/abs/2003.07496" target="_blank">arXiv</a>]
                            [<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Song_DEPARA_Deep_Attribution_CVPR_2020_supplemental.pdf" target="_blank">supp</a>]
                            [<a href="https://github.com/zju-vipa/DEPARA" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{song2020depara,
  title={Depara: Deep attribution graph for deep knowledge transferability},
  author={Song, Jie and Chen, Yixin and Ye, Jingwen and Wang, Xinchao and Shen, Chengchao and Mao, Feng and Song, Mingli},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3922--3930},
  year={2020}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Exploring the intrinsic interconnections between the knowledge encoded in PRe-trained Deep Neural Networks (PR-DNNs) of heterogeneous tasks sheds light on their mutual transferability, and consequently enables knowledge transfer from one task to another so as to reduce the training effort of the latter. In this paper, we propose the DEeP Attribution gRAph (DEPARA) to investigate the transferability of knowledge learned from PR-DNNs. In DEPARA, nodes correspond to the inputs and are represented by their vectorized attribution maps with regards to the outputs of the PR-DNN. Edges denote the relatedness between inputs and are measured by the similarity of their features extracted from the PR-DNN. The knowledge transferability of two PR-DNNs is measured by the similarity of their corresponding DEPARAs. We apply DEPARA to two important yet under-studied problems in transfer learning: pre-trained model selection and layer selection. Extensive experiments are conducted to demonstrate the effectiveness and superiority of the proposed method in solving both these problems.</p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/zju-vipa/DEPARA.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                    </dl>
                </div>

                <div class="publication">
                    <dl class="description">
                        <div class="figure"><img src="images/song2019deep.png"></img></div>
                        <dt class="ptitle">Deep Model Transferability from Attribution Maps</dt>
                        <dd>Jie Song, Yixin Chen, Xinchao Wang, <b class='me'>Chengchao Shen</b>, Mingli Song</dd>
                        <dd>Advances in Neural Information Processing Systems (<b>NeurIPS</b>, <b>CCF A</b>), 2019</dd>
                        <dd>
                            [<a href="https://papers.nips.cc/paper/2019/file/e94fe9ac8dc10dd8b9a239e6abee2848-Paper.pdf" target="_blank">paper</a>]
                            [<a href="https://arxiv.org/abs/1909.11902" target="_blank">arXiv</a>]
                            [<a href="https://papers.nips.cc/paper/2019/file/e94fe9ac8dc10dd8b9a239e6abee2848-Supplemental.zip" target="_blank">supp</a>]
                            [<a href="https://github.com/zju-vipa/TransferbilityFromAttributionMaps" target="_blank">code</a>]
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.bibref').slideToggle()">bibtex</a>]
                                <div class="bibref pre-white-space"  style="overflow: hidden; display: none;">  
@inproceedings{song2019deep,
  title={Deep model transferability from attribution maps},
  author={Song, Jie and Chen, Yixin and Wang, Xinchao and Shen, Chengchao and Song, Mingli},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
                                </div>
                            </div>
                            <div class="link2">[<a class="fakelink" onclick="$(this).siblings('.abstract').slideToggle()">abstract</a>]
                                <div class="abstract"  style="overflow: hidden; display: none;">  
                                    <p>Exploring the transferability between heterogeneous tasks sheds light on their intrinsic interconnections, and consequently enables knowledge transfer from one task to another so as to reduce the training effort of the latter. In this paper, we propose an embarrassingly simple yet very efficacious approach to estimating the transferability of deep networks, especially those handling vision tasks. Unlike the seminal work of taskonomy that relies on a large number of annotations as supervision and is thus computationally cumbersome, the proposed approach requires no human annotations and imposes no constraints on the architectures of the networks. This is achieved, specifically, via projecting deep networks into a model space, wherein each network is treated as a point and the distances between two points are measured by deviations of their produced attribution maps. The proposed approach is several-magnitude times faster than taskonomy, and meanwhile preserves a task-wise topological structure highly similar to the one obtained by taskonomy. </p>
                                </div>
                            </div>
                            <img src="https://img.shields.io/github/stars/zju-vipa/TransferbilityFromAttributionMaps.svg?style=social&amp;label=Star" alt="GitHub stars"
                                style="text-align:center;vertical-align:middle">
                        </dd>
                    </dl>
                </div>

            </div>

            <!-- ---------------- Education ---------------- -->
            <!--
            <div class="education" id="education">
                <h3 class="title">教育经历</h3>

                <ul>
                    <li>2016.09 - 2021.06, 工学博士, 计算机科学与技术, 浙江大学</li>
                </ul>
                
            </div>
            -->

            <!-- ----------------- Services ----------------- -->
            <div class="services" id="services">
                <h3 class="title">学术服务</h3>
                <!-- <b style="font-size: 20px;">Conference Reviewer</b>: -->
                <b style="font-size: 20px;">会议审稿</b>:
                <!-- <h4 style="font-size: 20px; padding-top: -10px;">Program Committee: </b> -->
                <ul>
                    <li>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021, 2022, 2023</li>
                    <li>The IEEE International Conference on Computer Vision (ICCV), 2019, 2021, 2023</li>
                    <li>The European Conference on Computer Vision (ECCV), 2022</li>
                    <li>AAAI Conference on Artificial Intelligence (AAAI), 2020, 2021</li>
                    <li>Advances in Neural Information Processing Systems (NeurIPS), 2021, 2022, 2023</li>
                    <li>International Conference on Learning Representations (ICLR), 2022, 2023</li>
                    <li>International Conference on Machine Learning (ICML), 2022, 2023</li>
                    <li>International Joint Conference on Artificial Intelligence (IJCAI), 2022</li>
                    <li>The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2022</li>
                    <li>The European Conference on Artificial Intelligence (ECAI), 2023</li>
                </ul>

                <b style="font-size: 20px;">期刊审稿</b>:
                <ul>
                    <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                    <li>IEEE Transactions on Image Processing (TIP)</li>
                    <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
                </ul>

            </div>

            <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fchengchaoshen.github.io&count_bg=%23126DE4&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/>

            

    </div>

        
    <br>
    <br>
    </body>

</html>
