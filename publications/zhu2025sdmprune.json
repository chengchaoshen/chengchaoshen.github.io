{
    "data_area": [
        "efficiency",
        "distillation",
        "llm"
    ],
    "paper": {
        "image": "images/zhu2025sdmprune.png",
        "title": "SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models",
        "authors": [
            "Hourun Zhu",
            {
                "name": "Chengchao Shen",
                "is_me": true
            }
        ],
        "venue": {
            "name": "Arxiv",
            "short_name": "",
            "ranking": "",
            "year": 2025
        },
        "links": {
            "code": "https://github.com/visresearch/SDMPrune"
        },
        "bibtex": "\n@article{zhu2025sdmprune,\n    author  = {Zhu, Hourun and Shen, Chengchao},\n    title   = {SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models},\n    year    = {2025},\n}\n",
        "abstract": "In spite of strong performance achieved by LLMs, the costs of their deployment are unaffordable. For the compression of LLMs, gradient-based pruning methods present promising effectiveness. However, in these methods, the gradient computation with one-hot labels ignore the potential predictions on other words, thus missing key information for generative capability of the original model. To address this issue, we introduce a self-distillation loss during the pruning phase (rather than post-training) to fully exploit the predictions of the original model, thereby obtaining more accurate gradient information for pruning. Moreover, we find that, compared to attention modules, the predictions of LLM are less sensitive to multilayer perceptron (MLP) modules, which take up more than 5Ã— parameters (LLaMA3.2-1.2B). To this end, we focus on the pruning of MLP modules, to significantly compress LLM without obvious performance degradation. Experimental results on extensive zero-shot benchmarks demonstrate that our method significantly outperforms existing pruning methods. Furthermore, our method achieves very competitive performance among 1B-scale open source LLMs.",
        "github": {
            "user": "visresearch",
            "repo": "SDMPrune"
        }
    }
}