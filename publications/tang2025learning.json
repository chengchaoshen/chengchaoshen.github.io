{
    "data_area": [
        "efficiency",
        "multimodal"
    ],
    "paper": {
        "image": "images/tang2025compact.png",
        "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models",
        "authors": [
            "Hao Tang",
            {
                "name": "Chengchao Shen*",
                "is_me": true
            }
        ],
        "venue": {
            "name": "Arxiv",
            "short_name": "",
            "ranking": "",
            "year": 2025
        },
        "links": {
            "arxiv": "https://arxiv.org/abs/2506.07138",
            "code": "https://github.com/visresearch/LLaVA-STF",
            "weights": "https://huggingface.co/visresearch/LLaVA-STF/tree/main"
        },
        "bibtex": "\n@article{tang2025compact,\n    author  = {Tang, Hao and Shen, Chengchao},\n    title   = {Learning Compact Vision Tokens for Efficient Large Multimodal Models},\n    journal = {arXiv preprint arXiv:2506.07138},\n    year    = {2025},\n}\n",
        "abstract": "Large multimodal models (LMMs) suffer significant computational challenges due to the high cost of Large Language Models (LLMs) and the quadratic complexity of processing long vision token sequences. In this paper, we explore the spatial redundancy among vision tokens and shorten the length of vision token sequences for inference acceleration. Specifically, we propose a Spatial Token Fusion (STF) method to learn compact vision tokens for short vision token sequence, where spatial-adjacent tokens are fused into one. Meanwhile, weight-frozen vision encoder can not well adapt to the demand of extensive downstream vision-language tasks. To this end, we further introduce a Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features for the reduced token sequence. Overall, we combine STF and MBTF module to balance token reduction and information preservation, thereby improving inference efficiency without sacrificing multimodal reasoning capabilities. Experimental results demonstrate that our method based on LLaVA-1.5 achieves comparable or even superior performance to the baseline on 8 popular vision-language benchmarks with only 25% vision tokens of baseline.",
        "github": {
            "user": "visresearch",
            "repo": "LLaVA-STF"
        }
    }
}